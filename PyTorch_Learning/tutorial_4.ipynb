{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# <center style=\"font-family: consolas; font-size: 32px; font-weight: bold;\">PyTorch</center>\n",
    "<p><center style=\"color:#949494; font-family: consolas; font-size: 20px;\">https://pytorch.org/</center></p>\n",
    "\n",
    "***\n",
    "\n",
    "# <center style=\"font-family: consolas; font-size: 32px; font-weight: bold;\">D. Dataload</center>"
   ],
   "id": "a9adda69db6e40cf"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-12T14:03:30.058818Z",
     "start_time": "2025-02-12T14:03:28.386779Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<font face='consolas' size=3 color=#f8000b>1. Data load</font>",
   "id": "f13d754c2bd297c7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T14:03:59.642372Z",
     "start_time": "2025-02-12T14:03:59.638081Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_list):\n",
    "        self.data_list = data_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data_list[index]\n",
    "\n",
    "my_dataset = MyDataset([1, 2, 3, 4, 5])\n",
    "\n",
    "my_dataloader = DataLoader(my_dataset,\n",
    "                           batch_size=4,\n",
    "                           shuffle=True)\n",
    "\n",
    "for batch in my_dataloader:\n",
    "    print(batch)"
   ],
   "id": "f53277d021432b48",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4, 5, 2, 1])\n",
      "tensor([3])\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<font face='consolas' size=3 color=#f8000b>2. Data preprocessing</font>",
   "id": "ccf35b08c390a5c0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T14:07:35.195590Z",
     "start_time": "2025-02-12T14:07:35.007035Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "data = [1, 2, 3, 4, 5]\n",
    "tensor = torch.tensor(data)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# transformed_data = transform(tensor)"
   ],
   "id": "2d91336acd77aa61",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Tensor is not a torch image.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[12], line 11\u001B[0m\n\u001B[0;32m      4\u001B[0m tensor \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(data)\n\u001B[0;32m      6\u001B[0m transform \u001B[38;5;241m=\u001B[39m transforms\u001B[38;5;241m.\u001B[39mCompose([\n\u001B[0;32m      7\u001B[0m     transforms\u001B[38;5;241m.\u001B[39mResize((\u001B[38;5;241m224\u001B[39m, \u001B[38;5;241m224\u001B[39m)),\n\u001B[0;32m      8\u001B[0m     transforms\u001B[38;5;241m.\u001B[39mToTensor()\n\u001B[0;32m      9\u001B[0m ])\n\u001B[1;32m---> 11\u001B[0m transformed_data \u001B[38;5;241m=\u001B[39m transform(tensor)\n",
      "File \u001B[1;32mD:\\ProgramData\\anaconda3\\envs\\machine_learning\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001B[0m, in \u001B[0;36mCompose.__call__\u001B[1;34m(self, img)\u001B[0m\n\u001B[0;32m     93\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, img):\n\u001B[0;32m     94\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransforms:\n\u001B[1;32m---> 95\u001B[0m         img \u001B[38;5;241m=\u001B[39m t(img)\n\u001B[0;32m     96\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m img\n",
      "File \u001B[1;32mD:\\ProgramData\\anaconda3\\envs\\machine_learning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\ProgramData\\anaconda3\\envs\\machine_learning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mD:\\ProgramData\\anaconda3\\envs\\machine_learning\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:354\u001B[0m, in \u001B[0;36mResize.forward\u001B[1;34m(self, img)\u001B[0m\n\u001B[0;32m    346\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, img):\n\u001B[0;32m    347\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    348\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[0;32m    349\u001B[0m \u001B[38;5;124;03m        img (PIL Image or Tensor): Image to be scaled.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    352\u001B[0m \u001B[38;5;124;03m        PIL Image or Tensor: Rescaled image.\u001B[39;00m\n\u001B[0;32m    353\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 354\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mresize(img, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msize, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minterpolation, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_size, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mantialias)\n",
      "File \u001B[1;32mD:\\ProgramData\\anaconda3\\envs\\machine_learning\\Lib\\site-packages\\torchvision\\transforms\\functional.py:465\u001B[0m, in \u001B[0;36mresize\u001B[1;34m(img, size, interpolation, max_size, antialias)\u001B[0m\n\u001B[0;32m    459\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m max_size \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(size) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m    460\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    461\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmax_size should only be passed if size specifies the length of the smaller edge, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    462\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mi.e. size should be an int or a sequence of length 1 in torchscript mode.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    463\u001B[0m         )\n\u001B[1;32m--> 465\u001B[0m _, image_height, image_width \u001B[38;5;241m=\u001B[39m get_dimensions(img)\n\u001B[0;32m    466\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(size, \u001B[38;5;28mint\u001B[39m):\n\u001B[0;32m    467\u001B[0m     size \u001B[38;5;241m=\u001B[39m [size]\n",
      "File \u001B[1;32mD:\\ProgramData\\anaconda3\\envs\\machine_learning\\Lib\\site-packages\\torchvision\\transforms\\functional.py:78\u001B[0m, in \u001B[0;36mget_dimensions\u001B[1;34m(img)\u001B[0m\n\u001B[0;32m     76\u001B[0m     _log_api_usage_once(get_dimensions)\n\u001B[0;32m     77\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(img, torch\u001B[38;5;241m.\u001B[39mTensor):\n\u001B[1;32m---> 78\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F_t\u001B[38;5;241m.\u001B[39mget_dimensions(img)\n\u001B[0;32m     80\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m F_pil\u001B[38;5;241m.\u001B[39mget_dimensions(img)\n",
      "File \u001B[1;32mD:\\ProgramData\\anaconda3\\envs\\machine_learning\\Lib\\site-packages\\torchvision\\transforms\\_functional_tensor.py:19\u001B[0m, in \u001B[0;36mget_dimensions\u001B[1;34m(img)\u001B[0m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_dimensions\u001B[39m(img: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m List[\u001B[38;5;28mint\u001B[39m]:\n\u001B[1;32m---> 19\u001B[0m     _assert_image_tensor(img)\n\u001B[0;32m     20\u001B[0m     channels \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m img\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m2\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m img\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m3\u001B[39m]\n\u001B[0;32m     21\u001B[0m     height, width \u001B[38;5;241m=\u001B[39m img\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m2\u001B[39m:]\n",
      "File \u001B[1;32mD:\\ProgramData\\anaconda3\\envs\\machine_learning\\Lib\\site-packages\\torchvision\\transforms\\_functional_tensor.py:15\u001B[0m, in \u001B[0;36m_assert_image_tensor\u001B[1;34m(img)\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_assert_image_tensor\u001B[39m(img: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m     14\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _is_tensor_a_torch_image(img):\n\u001B[1;32m---> 15\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTensor is not a torch image.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mTypeError\u001B[0m: Tensor is not a torch image."
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "\n",
    "\n",
    "# normalized_image = normalize(image)"
   ],
   "id": "459a52113e0d6da1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(30)\n",
    "])\n",
    "\n",
    "# 对图像进行增强\n",
    "# transformed_image = transform(image)"
   ],
   "id": "bc78e167462711f2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "ccf0e8ec6c3a1313"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
